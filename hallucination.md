# Hallucination & Limitations

AI models can sometimes generate information that sounds plausible but is actually incorrect or fabricated. This phenomenon is called "hallucination."

## What causes hallucinations?

Hallucinations occur because AI models are trained to generate text that seems likely based on patterns in their training data, not because they have access to a database of facts.

### Common scenarios:

- **Making up citations**: The model invents academic papers or sources
- **Fabricating details**: Adding specific dates, names, or numbers that are incorrect
- **Contradictions**: Providing information that conflicts with earlier statements
- **Overconfidence**: Stating uncertain information with absolute certainty

## Limitations to be aware of

1. **Knowledge cutoff**: Models only know information up to their training date
2. **No real-time data**: Cannot access current events or live information
3. **Context window**: Limited memory of conversation history
4. **Bias**: Can reflect biases present in training data
5. **Mathematical reasoning**: May struggle with complex calculations

## Best practices

- Always verify critical information
- Ask for sources and check them
- Use temperature settings to control creativity
- Break complex tasks into smaller steps
- Provide clear, specific instructions
